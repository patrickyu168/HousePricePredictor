# -*- coding: utf-8 -*-
"""House Price Predictor

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1R24xyTfejLH4ajha5liF8ppgAae1VTa2
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor

trainDF = pd.read_csv('train.csv') #1460 rows by 81 columns
trainDF.head()

trainDF.describe()

import altair as alt

alt.Chart(trainDF).mark_bar().encode(
    alt.X("SalePrice", bin=True),
    y='count()',
)

"""DATA CLEANING

"""

colsWithNA = [col for col in trainDF.columns if trainDF[col].isnull().any()]
colsWithNA #a lot of these are features that aren't usually present in houses, hence why there are NA values
for col in colsWithNA: #dropping 19 columns
  trainDF = trainDF.drop(col, axis = 1)
colsWithNA
trainDF

#turning all categorical data into numerical values
copy = trainDF.copy()

categoricalData = ['MSZoning', 'Street', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 
                                     'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'ExterQual', 'ExterCond', 
                                     'Foundation', 'Heating', 'HeatingQC', 'CentralAir', 'KitchenQual', 'Functional', 'SaleType', 'SaleCondition', 'PavedDrive']
#trainDFcategorical = trainDF.loc[:, categoricalData] 
#turning them into numerical values using the ordinal encoder
from sklearn.preprocessing import OrdinalEncoder
ordinal_encoder = OrdinalEncoder()
housing_cat_encoded = ordinal_encoder.fit_transform(copy.loc[:, categoricalData])
copy.loc[:, categoricalData] = ordinal_encoder.fit_transform(copy.loc[:, categoricalData])

#one hot encoding
from sklearn.preprocessing import OneHotEncoder
cat_encoder = OneHotEncoder()
housing_cat_1hot = cat_encoder.fit_transform(copy.loc[:, categoricalData])

#the categories for each data parametre
cat_encoder.categories_

#standardising all values
colNames = copy.columns
from sklearn.preprocessing import StandardScaler
copy = pd.DataFrame(StandardScaler().fit_transform(copy))
copy.columns = colNames
copy

correlationMatrix = trainDF.corr()
correlationMatrix['SalePrice'].sort_values(ascending = False)

#correlation of all parametres relative to the sale price
import seaborn as sns
plt.subplots(figsize=(40,22))
sns.heatmap(trainDF.corr())
#we basically just need to pay attention to the right most row and see what features are most correlated with sale price. clearly, overall 
#quality is the most important, while things like GrLivingArea and GarageArea are also fairly important.

#pca
df = trainDF.select_dtypes(include= 'number')
prices = df['SalePrice']
df
features = df[['OverallQual', 'GrLivArea', 'GarageArea', 'GarageCars', '1stFlrSF']]
features

sns.pairplot(df[['SalePrice', 'OverallQual', 'GrLivArea', 'GarageArea', 'GarageCars', '1stFlrSF']],size=1.5)
#it's really the top row that we need to pay attention to. judging by most of these plots, it becomes clear that most of these features 
#positively correlated with sale price; in other words.

from sklearn.preprocessing import StandardScaler
x = StandardScaler().fit_transform(df.loc[:, ['OverallQual', 'GrLivArea', 'GarageArea', 'GarageCars', '1stFlrSF']])
x

from sklearn.decomposition import PCA
pca = PCA(n_components=2)
principalComponents = pca.fit_transform(x)
principalDf = pd.DataFrame(data = principalComponents
             , columns = ['principal component 1', 'principal component 2'])

fig = plt.figure(figsize = (8,8))
ax = fig.add_subplot(1,1,1) 
ax.set_xlabel('Principal Component 1', fontsize = 15)
ax.set_ylabel('Principal Component 2', fontsize = 15)
ax.set_title('2 component PCA', fontsize = 20)
X = principalDf.loc[:, 'principal component 1']
Y = principalDf.loc[:, 'principal component 2']
ax.scatter(X, Y)

pca.get_params()

X_1 = X[df['SalePrice'] <= 129975]
Y_1 = Y[df['SalePrice'] <= 129975]

X_2 = X[df['SalePrice'] > 129975]
X_2 = X[df['SalePrice'] <= 163000]
Y_2 = Y[df['SalePrice'] > 129975]
Y_2 = Y[df['SalePrice'] <= 163000]

X_3 = X[df['SalePrice'] > 163000]
X_3 = X[df['SalePrice'] <= 214000]
Y_3 = Y[df['SalePrice'] > 163000]
Y_3 = Y[df['SalePrice'] <= 214000]

X_4 = X[df['SalePrice'] > 214000]
Y_4 = Y[df['SalePrice'] > 214000]

fig = plt.figure(figsize = (8,8))
ax = fig.add_subplot(1,1,1) 
ax.set_xlabel('Principal Component 1', fontsize = 15)
ax.set_ylabel('Principal Component 2', fontsize = 15)
ax.set_title('2 component PCA', fontsize = 20)

ax.scatter(X_1, Y_1, color = 'red')
ax.scatter(X_2, Y_2, color = 'blue')
ax.scatter(X_3, Y_3, color = 'green')
ax.scatter(X_4, Y_4, color = 'yellow')

fig = plt.figure(figsize = (8,8))
ax = fig.add_subplot(1,1,1) 
ax.set_xlabel('Principal Component 1', fontsize = 15)
ax.set_ylabel('Principal Component 2', fontsize = 15)
ax.set_title('Cheap Homes PCA', fontsize = 20)
ax.scatter(X_1, Y_1, color = 'red')

fig = plt.figure(figsize = (8,8))
ax = fig.add_subplot(1,1,1) 
ax.set_xlabel('Principal Component 1', fontsize = 15)
ax.set_ylabel('Principal Component 2', fontsize = 15)
ax.set_title('Modest Homes PCA', fontsize = 20)
ax.scatter(X_2, Y_2, color = 'blue')

fig = plt.figure(figsize = (8,8))
ax = fig.add_subplot(1,1,1) 
ax.set_xlabel('Principal Component 1', fontsize = 15)
ax.set_ylabel('Principal Component 2', fontsize = 15)
ax.set_title('Middle Homes PCA', fontsize = 20)
ax.scatter(X_3, Y_3, color = 'green')

fig = plt.figure(figsize = (8,8))
ax = fig.add_subplot(1,1,1) 
ax.set_xlabel('Principal Component 1', fontsize = 15)
ax.set_ylabel('Principal Component 2', fontsize = 15)
ax.set_title('Expensive Homes PCA', fontsize = 20)
ax.scatter(X_4, Y_4, color = 'yellow')
#less clustering = more variance

"""DATA OPTIMISATION"""

#Need to tune the hyperparametres
#Will use randomised search cross validation
#https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74 
#https://medium.com/@ODSC/optimizing-hyperparameters-for-random-forest-algorithms-in-scikit-learn-d60b7aa07ead

# Possible Hyperparameters

# number of trees
n_estimators = [500, 800, 1500, 2500, 5000]
# max number of features to consider at every split
max_features = ['auto', 'sqrt', 'log2']
# max number of levels in tree
max_depth = [10, 20, 30, 40, 50]
max_depth.append(None)
# minimum number of samples required to split a node
min_samples_split = [2,5,10, 15, 20]
# minimum number of samples required at each leaf node
min_samples_leaf = [1, 2, 5, 10, 15]

# Dictionary of the candidate hyperparameter values
grid_param = {'n_estimators': n_estimators, 'max_features': max_features, 'max_depth': max_depth, 'min_samples_split': min_samples_split, 'min_samples_leaf': min_samples_leaf}

# Define the model type (random forest regressor)
from sklearn.model_selection import RandomizedSearchCV
RFR = RandomForestRegressor(random_state=1)
RFR_random = RandomizedSearchCV(estimator = RFR, param_distributions = grid_param, n_iter = 10, cv = 5, verbose = 2, random_state = 42, n_jobs = -1)

#cross validation
#used to find the most important features for pca
#.feature_importances
#clf 
#https://www.analyticsvidhya.com/blog/2021/07/data-leakage-and-its-effect-on-the-performance-of-an-ml-model/
#https://towardsdatascience.com/automated-feature-engineering-in-python-99baf11cc219 
#https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html

# Fit the cross_validate object to the data frames containing features and labels

features = df[['OverallQual', 'GrLivArea', 'GarageArea', 'GarageCars', '1stFlrSF']]  # features: list of chosen features
labels = df['SalePrice'] # sale price feature
RFR_random.fit(features, labels)
#features is our x train, labels is our y train, where x train are the features, y train is sale price
print(RFR_random.best_params_)

from sklearn.model_selection import cross_val_score
cross_val_score(RFR_random, features, labels, scoring= 'neg_root_mean_squared_error')
CVSResult = [-30095.52373308, -36833.12847327, -36796.1809824 , -28227.22184937,
       -35408.67273637] 
#large difference between validation and test score - high variance, overfitting

"""DATA PREDICTION

"""

RFR2 = RandomForestRegressor(n_estimators = 2500, min_samples_split = 10, min_samples_leaf = 1, max_features = 'sqrt', max_depth = 50)
RFR2.fit(features, labels)

testDF = pd.read_csv('test.csv')
test_features = testDF[['OverallQual', 'GrLivArea', 'GarageArea', 'GarageCars', '1stFlrSF']]

print(test_features.shape)
print(testDF.shape)
test_features #1459 rows x 5 columns
test_features = test_features.fillna(df.mean())
print(test_features.shape)

y_pred = RFR2.predict(test_features)
y_pred.shape

Id = testDF.iloc[:,0:1]
SalePrice = pd.DataFrame(y_pred)
Id1 = Id.join(SalePrice)
Id1.columns = ['Id', 'SalePrice']
Id1
Id1.set_index('Id', inplace = True)
Id1.to_csv('Output.csv')

trainDF['Id'][0]
y_pred[0]
print(y_pred.shape)
print(testDF.shape)

#split the train dataset and find prediction accuracy score

from sklearn import linear_model
lr = linear_model.LinearRegression()
x_train = df[['OverallQual', 'GrLivArea', 'GarageArea', 'GarageCars', '1stFlrSF']]  # features: list of chosen features
y_train = df['SalePrice'] # sale price feature
model = lr.fit(x_train, y_train)

x_test = 
y_test = 
predictions = model.predict(x_test)

from sklearn.metrics import mean_absolute_error, mean_squared_error
from math import sqrt

print("MAE is:", mean_absolute_error(y_test, predictions))
print("MSE is:", mean_squared_error(y_test, predictions))
print("RMSE is:", sqrt(mean_squared_error(y_test, predictions)))

plt.scatter(predictions, y_test, alpha = 0.7, color = 'b')
plt.xlabel("Predicted Price")
plt.ylabel("Actual Price") 
plt.title("Linear Regression Model of Sale Price")

trainDF.head()